{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcY9y56n86vn"
   },
   "source": [
    "# Tutorial: creación de ETLs con PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqQVz5s686vq"
   },
   "source": [
    "## 2. Configuración Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "os1iJYmu86vt"
   },
   "outputs": [],
   "source": [
    "# Configuración servidor base de datos transaccional\n",
    "db_user = 'Estudiante_111_202315'\n",
    "db_psswd = 'aabb1122'\n",
    "source_db_connection_string = 'jdbc:mysql://157.253.236.116:8080/WWImportersTransactional'\n",
    "dest_db_connection_string = 'jdbc:mysql://157.253.236.116:8080/Estudiante_111_202315'\n",
    "\n",
    "\n",
    "\n",
    "db_connection_string = 'jdbc:mysql://157.253.236.116:8080/WWImportersTransactional'\n",
    "# Driver de conexion\n",
    "path_jar_driver = 'C:\\Program Files (x86)\\MySQL\\Connector J 8.0\\mysql-connector-java-8.0.28.jar'\n",
    "\n",
    "PATH='./'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HAGk_V1986vu"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "from pyspark.sql import functions as f, SparkSession, types as t\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.functions import udf, col, length, isnan, when, count, regexp_replace\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import TimestampType, DateType\n",
    "import re\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col, date_format, to_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TQ3DM_Xf86vv",
    "outputId": "f2cdb157-a92d-4edc-a174-6f3bd09b9885"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\estudiante\\anaconda3\\envs\\Tutoriales\\lib\\site-packages\\pyspark\\sql\\context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "#Configuración de la sesión\n",
    "conf=SparkConf() \\\n",
    "    .set('spark.driver.extraClassPath', path_jar_driver)\n",
    "\n",
    "spark_context = SparkContext(conf=conf)\n",
    "sql_context = SQLContext(spark_context)\n",
    "spark = sql_context.sparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buA7s-vL86vw"
   },
   "source": [
    "### Conexión y carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_QZ-j9q86vw"
   },
   "source": [
    "Se define la función para conexión y cargue de dataframes desde la base de datos origen y luego la función para guardar un dataframe en una tabla de la base de datos destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Mhy0jbh_86vw"
   },
   "outputs": [],
   "source": [
    "def obterner_dataframe_desde_csv(_PATH, _sep):\n",
    "    return spark.read.load(_PATH, format=\"csv\", sep=_sep, inferSchema=\"true\", header='true')\n",
    "\n",
    "def obtener_dataframe_de_bd(db_connection_string, sql, db_user, db_psswd):\n",
    "    df_bd = spark.read.format('jdbc')\\\n",
    "        .option('url', db_connection_string) \\\n",
    "        .option('dbtable', sql) \\\n",
    "        .option('user', db_user) \\\n",
    "        .option('password', db_psswd) \\\n",
    "        .option('driver', 'com.mysql.cj.jdbc.Driver') \\\n",
    "        .load()\n",
    "    return df_bd\n",
    "\n",
    "def guardar_db(db_connection_string, df, tabla, db_user, db_psswd):\n",
    "    df.select('*').write.format('jdbc') \\\n",
    "      .mode('append') \\\n",
    "      .option('url', db_connection_string) \\\n",
    "      .option('dbtable', tabla) \\\n",
    "      .option('user', db_user) \\\n",
    "      .option('password', db_psswd) \\\n",
    "      .option('driver', 'com.mysql.cj.jdbc.Driver') \\\n",
    "      .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhFsv4ba86vw"
   },
   "source": [
    "### Tabla Dimensional: TIPOTRANSACTION\n",
    "\n",
    "Fuente de datos:\n",
    "Tabla transactional TiposTransaction . Su fuente de datos es la tabla transaccional <i>TiposTransaccion</i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNKxFA-f86vx",
    "tags": []
   },
   "source": [
    "#### Extracción\n",
    "A continuación, nos conectamos a la base de datos y extraemos la información deseada por medio de SQL, cargandola en un DataFrame PySpark, es decir en memoria. Note que aquí se pueden renombrar los atributos con la estructura <i>nombreActual AS nuevoNombre</i>. De la tabla de personas, En este paso, solo nos interesan los empleados, por lo cual se hace un filtro por medio del WHERE, buscando las personas cuyo atributo EsVendedor sea igual a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IopBNYuc86vx",
    "outputId": "0589f8a9-527e-4b28-d096-a176efd48e97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+\n",
      "|ID_Tipo_transaccion_T|                Tipo|\n",
      "+---------------------+--------------------+\n",
      "|                    2|Customer Credit Note|\n",
      "|                    3|Customer Payment ...|\n",
      "|                    4|     Customer Refund|\n",
      "|                    5|    Supplier Invoice|\n",
      "|                    6|Supplier Credit Note|\n",
      "|                    7|Supplier Payment ...|\n",
      "|                    8|     Supplier Refund|\n",
      "|                    9|      Stock Transfer|\n",
      "|                   10|         Stock Issue|\n",
      "|                   11|       Stock Receipt|\n",
      "+---------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- ID_Tipo_transaccion_T: integer (nullable = true)\n",
      " |-- Tipo: string (nullable = true)\n",
      "\n",
      "Numero de registros 12\n"
     ]
    }
   ],
   "source": [
    "sql_tipostrans = '''(SELECT DISTINCT TipoTransaccionID AS ID_Tipo_transaccion_T , TipoTransaccionNombre AS Tipo FROM WWImportersTransactional.TiposTransaccion) AS Temp_empleados'''\n",
    "tipos_trans_df = obtener_dataframe_de_bd(source_db_connection_string, sql_tipostrans, db_user, db_psswd)\n",
    "tipos_trans_df.show(10)\n",
    "tipos_trans_df.printSchema()\n",
    "\n",
    "print(f'Numero de registros {tipos_trans_df.count()}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de registros 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'Numero de registros {tipos_trans_df.count()}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtE61qk986vx"
   },
   "source": [
    "#### Transformación\n",
    "Recuerde que, puede hacer uso de selectExpr, filter, where entre otras de PySpark para modificar los datos cargados. Por ejemplo, el siguiente código utiliza <i>selectExpr</i> para renombrar la columna ID_Empleado por ID_Empleado_T, esta es la convención que vamos a utilizar: \"_T\" para indicar que el ID es el que estaba en la base de datos transaccional y \"_DWH\" para indicar que son ID's propios de la bodega. Usamos withColumn y monotonicallu_increasing_id para crear un ID acumulativo para cada registro en el dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMACION\n",
    "#Ninguna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9Oou0g986vy"
   },
   "source": [
    "#### Carga\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qICUWYCa86vy"
   },
   "outputs": [],
   "source": [
    "# CARGUE\n",
    "guardar_db(dest_db_connection_string, tipos_trans_df,'Estudiante_111_202315.TipoTransaccion', db_user, db_psswd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6k37LtO86vy"
   },
   "source": [
    "#### Validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------------+--------------------+\n",
      "|ID_Tipo_transaccion_DWH|ID_Tipo_transaccion_T|                Tipo|\n",
      "+-----------------------+---------------------+--------------------+\n",
      "|                      6|                    2|Customer Credit Note|\n",
      "|                      7|                    3|Customer Payment ...|\n",
      "|                      8|                    4|     Customer Refund|\n",
      "|                      9|                    5|    Supplier Invoice|\n",
      "|                     10|                    6|Supplier Credit Note|\n",
      "+-----------------------+---------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Numero de registros 12\n"
     ]
    }
   ],
   "source": [
    "test = obtener_dataframe_de_bd(dest_db_connection_string, \"Estudiante_111_202315.TipoTransaccion\", db_user, db_psswd)\n",
    "test.show(5)\n",
    "print(f'Numero de registros {test.count()}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension: CIUDAD\n",
    "Su fuente de datos es una combinación de las tablas transaccionales <i>paises, provinciasEstados y ciudades</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID_ciudad_T', 'NombreCiudad', 'ID_EstadoProvincia', 'Poblacion'] ['ID_Pais', 'Nombre', 'Continente', 'Region', 'Subregion'] ['ID_EstadoProvincia', 'NombreEstadoProvincia', 'TerritorioVentas', 'ID_Pais']\n"
     ]
    }
   ],
   "source": [
    "sql_paises = '''(SELECT DISTINCT ID_Pais, Nombre, Continente, Region, Subregion FROM WWImportersTransactional.Paises) AS Temp_paises'''\n",
    "sql_provincias_estados = '''(SELECT DISTINCT ID_EstadosProvincias AS ID_EstadoProvincia, NombreEstadoProvincia, TerritorioVentas, ID_Pais FROM WWImportersTransactional.EstadosProvincias) AS Temp_estados_provincias'''\n",
    "sql_ciudades = '''(SELECT DISTINCT ID_ciudad as ID_ciudad_T, NombreCiudad, ID_EstadoProvincia, Poblacion FROM WWImportersTransactional.Ciudades) AS Temp_ciudades'''\n",
    "\n",
    "paises_df = obtener_dataframe_de_bd(source_db_connection_string, sql_paises, db_user, db_psswd)\n",
    "provincias_estados_df = obtener_dataframe_de_bd(source_db_connection_string, sql_provincias_estados, db_user, db_psswd)\n",
    "ciudades_df = obtener_dataframe_de_bd(source_db_connection_string, sql_ciudades, db_user, db_psswd)\n",
    "\n",
    "print(ciudades.columns, paises.columns, provincias_estados.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID_Ciudad_DWH: long (nullable = false)\n",
      " |-- ID_ciudad_T: integer (nullable = true)\n",
      " |-- NombreCiudad: string (nullable = true)\n",
      " |-- Continente: string (nullable = true)\n",
      " |-- Pais: string (nullable = true)\n",
      " |-- Poblacion: long (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- TerritorioVentas: string (nullable = true)\n",
      " |-- NombreEstadoProvincia: string (nullable = true)\n",
      " |-- Subregion: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(ID_Ciudad_DWH=1, ID_ciudad_T=49, NombreCiudad='Absecon', Continente='North America', Pais='United States', Poblacion=8411, Region='Americas', TerritorioVentas='Mideast', NombreEstadoProvincia='New Jersey', Subregion='Northern America'),\n",
       " Row(ID_Ciudad_DWH=2, ID_ciudad_T=150, NombreCiudad='Adelphia', Continente='North America', Pais='United States', Poblacion=None, Region='Americas', TerritorioVentas='Mideast', NombreEstadoProvincia='New Jersey', Subregion='Northern America'),\n",
       " Row(ID_Ciudad_DWH=3, ID_ciudad_T=336, NombreCiudad='Albion', Continente='North America', Pais='United States', Poblacion=None, Region='Americas', TerritorioVentas='Mideast', NombreEstadoProvincia='New Jersey', Subregion='Northern America')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRANSFORMACION\n",
    "ciudades_df = ciudades_df.join(provincias_estados_df, how = 'left', on = 'ID_EstadoProvincia')\n",
    "ciudades_df = ciudades_df.join(paises_df, how = 'left', on = 'ID_Pais')\n",
    "ciudades_df = ciudades_df.coalesce(1).withColumn('ID_Ciudad_DWH', f.monotonically_increasing_id() + 1)\n",
    "ciudades_df = ciudades_df.select('ID_Ciudad_DWH','ID_ciudad_T','NombreCiudad','Continente','Nombre','Poblacion',\n",
    "                          'Region','TerritorioVentas','NombreEstadoProvincia','Subregion') \\\n",
    "                    .withColumnRenamed('Nombre','Pais')\n",
    "\n",
    "ciudades_df.printSchema()\n",
    "\n",
    "ciudades_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGA\n",
    "guardar_db(dest_db_connection_string, ciudades_df,'Estudiante_111_202315.Ciudad', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+-------------+-------------+---------+--------+----------------+---------------------+----------------+\n",
      "|ID_Ciudad_DWH|ID_ciudad_T|NombreCiudad|   Continente|         Pais|Poblacion|  Region|TerritorioVentas|NombreEstadoProvincia|       Subregion|\n",
      "+-------------+-----------+------------+-------------+-------------+---------+--------+----------------+---------------------+----------------+\n",
      "|            1|         49|     Absecon|North America|United States|     8411|Americas|         Mideast|           New Jersey|Northern America|\n",
      "|            2|        150|    Adelphia|North America|United States|     null|Americas|         Mideast|           New Jersey|Northern America|\n",
      "|            3|        336|      Albion|North America|United States|     null|Americas|         Mideast|           New Jersey|Northern America|\n",
      "|            4|        458|   Allamuchy|North America|United States|       78|Americas|         Mideast|           New Jersey|Northern America|\n",
      "|            5|        480|   Allendale|North America|United States|     6505|Americas|         Mideast|           New Jersey|Northern America|\n",
      "+-------------+-----------+------------+-------------+-------------+---------+--------+----------------+---------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Numero de registros 37940\n"
     ]
    }
   ],
   "source": [
    "#guardar_db(dest_db_connection_string, proveedores_df,'Estudiante_111_202315.Proveedor', db_user, db_psswd)\n",
    "test = obtener_dataframe_de_bd(dest_db_connection_string, \"Estudiante_111_202315.Ciudad\", db_user, db_psswd)\n",
    "test.show(5)\n",
    "print(f'Numero de registros {test.count()}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvj5E1GS86vz"
   },
   "source": [
    "### Dimension: PROVEEDOR\n",
    "Su fuente de datos es una combinación de las tablas transaccionales <i>proveedoresCopia, CategoriasProveedores y Personas</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5kFkHTD86vz"
   },
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "id": "FEW2mcMn86vz",
    "outputId": "a18be86c-fd81-4b56-de5c-bec1f41db118"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID_Proveedor_T: integer (nullable = true)\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- CategoriaProveedorID: integer (nullable = true)\n",
      " |-- PersonaContactoPrincipalID: integer (nullable = true)\n",
      " |-- Dias_pago: integer (nullable = true)\n",
      " |-- Codigo_postal: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- CategoriaProveedorID: integer (nullable = true)\n",
      " |-- Categoria: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ID_persona: integer (nullable = true)\n",
      " |-- Contacto_principal: string (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------------\n",
      " ID_Proveedor_T             | 4                    \n",
      " Nombre                     | Fabrikam, Inc.       \n",
      " CategoriaProveedorID       | 4                    \n",
      " PersonaContactoPrincipalID | 27                   \n",
      " Dias_pago                  | 30                   \n",
      " Codigo_postal              | 40351                \n",
      "-RECORD 1------------------------------------------\n",
      " ID_Proveedor_T             | 5                    \n",
      " Nombre                     | Graphic Design In... \n",
      " CategoriaProveedorID       | 2                    \n",
      " PersonaContactoPrincipalID | 29                   \n",
      " Dias_pago                  | 14                   \n",
      " Codigo_postal              | 64847                \n",
      "-RECORD 2------------------------------------------\n",
      " ID_Proveedor_T             | 7                    \n",
      " Nombre                     | Litware, Inc.        \n",
      " CategoriaProveedorID       | 5                    \n",
      " PersonaContactoPrincipalID | 33                   \n",
      " Dias_pago                  | 30                   \n",
      " Codigo_postal              | 95245                \n",
      "-RECORD 3------------------------------------------\n",
      " ID_Proveedor_T             | 9                    \n",
      " Nombre                     | Nod Publishers       \n",
      " CategoriaProveedorID       | 2                    \n",
      " PersonaContactoPrincipalID | 37                   \n",
      " Dias_pago                  | 7                    \n",
      " Codigo_postal              | 27906                \n",
      "-RECORD 4------------------------------------------\n",
      " ID_Proveedor_T             | 10                   \n",
      " Nombre                     | Northwind Electri... \n",
      " CategoriaProveedorID       | 3                    \n",
      " PersonaContactoPrincipalID | 39                   \n",
      " Dias_pago                  | 30                   \n",
      " Codigo_postal              | 7860                 \n",
      "-RECORD 5------------------------------------------\n",
      " ID_Proveedor_T             | 12                   \n",
      " Nombre                     | The Phone Company    \n",
      " CategoriaProveedorID       | 2                    \n",
      " PersonaContactoPrincipalID | 43                   \n",
      " Dias_pago                  | 30                   \n",
      " Codigo_postal              | 56732                \n",
      "-RECORD 6------------------------------------------\n",
      " ID_Proveedor_T             | 13                   \n",
      " Nombre                     | Woodgrove Bank       \n",
      " CategoriaProveedorID       | 7                    \n",
      " PersonaContactoPrincipalID | 45                   \n",
      " Dias_pago                  | 7                    \n",
      " Codigo_postal              | 94101                \n",
      "-RECORD 7------------------------------------------\n",
      " ID_Proveedor_T             | 2                    \n",
      " Nombre                     | Contoso, Ltd.        \n",
      " CategoriaProveedorID       | 2                    \n",
      " PersonaContactoPrincipalID | 23                   \n",
      " Dias_pago                  | -7                   \n",
      " Codigo_postal              | 98253                \n",
      "-RECORD 8------------------------------------------\n",
      " ID_Proveedor_T             | 6                    \n",
      " Nombre                     | Humongous Insurance  \n",
      " CategoriaProveedorID       | 9                    \n",
      " PersonaContactoPrincipalID | 31                   \n",
      " Dias_pago                  | -14                  \n",
      " Codigo_postal              | 37770                \n",
      "-RECORD 9------------------------------------------\n",
      " ID_Proveedor_T             | 8                    \n",
      " Nombre                     | Lucerne Publishing   \n",
      " CategoriaProveedorID       | 2                    \n",
      " PersonaContactoPrincipalID | 35                   \n",
      " Dias_pago                  | -30                  \n",
      " Codigo_postal              | 37659                \n",
      "-RECORD 10-----------------------------------------\n",
      " ID_Proveedor_T             | 1                    \n",
      " Nombre                     | A Datum Corporation  \n",
      " CategoriaProveedorID       | 2                    \n",
      " PersonaContactoPrincipalID | 21                   \n",
      " Dias_pago                  | -14                  \n",
      " Codigo_postal              | 46077                \n",
      "-RECORD 11-----------------------------------------\n",
      " ID_Proveedor_T             | 11                   \n",
      " Nombre                     | Trey Research        \n",
      " CategoriaProveedorID       | 8                    \n",
      " PersonaContactoPrincipalID | 41                   \n",
      " Dias_pago                  | -7                   \n",
      " Codigo_postal              | 57543                \n",
      "-RECORD 12-----------------------------------------\n",
      " ID_Proveedor_T             | 3                    \n",
      " Nombre                     | Consolidated Mess... \n",
      " CategoriaProveedorID       | 6                    \n",
      " PersonaContactoPrincipalID | 25                   \n",
      " Dias_pago                  | -30                  \n",
      " Codigo_postal              | 94101                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#EXTRACCION\n",
    "sql_provedores=  '''(SELECT DISTINCT ProveedorID as ID_Proveedor_T, NombreProveedor AS Nombre,CategoriaProveedorID, PersonaContactoPrincipalID, DiasPago AS Dias_pago, CodigoPostal AS Codigo_postal FROM WWImportersTransactional.proveedores) AS Temp_proveedores'''\n",
    "sql_cat_prov=  '''(SELECT DISTINCT CategoriaProveedorID, CategoriaProveedor AS Categoria FROM WWImportersTransactional.CategoriasProveedores) AS Temp_cat_proveedores'''\n",
    "sql_personas=  '''(SELECT DISTINCT ID_persona, NombreCompleto AS Contacto_principal FROM WWImportersTransactional.Personas) AS Temp_personas'''\n",
    "proveedores_df = obtener_dataframe_de_bd(source_db_connection_string, sql_provedores, db_user, db_psswd)\n",
    "cat_proveedores_df = obtener_dataframe_de_bd(source_db_connection_string, sql_cat_prov, db_user, db_psswd)\n",
    "personas_df = obtener_dataframe_de_bd(source_db_connection_string, sql_personas, db_user, db_psswd)\n",
    "proveedores_df.printSchema()\n",
    "cat_proveedores_df.printSchema()\n",
    "personas_df.printSchema()\n",
    "proveedores_df.count()\n",
    "proveedores_df.show(13, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddFhEOmL86vz"
   },
   "source": [
    "#### Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "id": "8un2p4so86v0",
    "outputId": "ba2758d9-aca3-4198-c13c-514bdbf51061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID_Proveedor_T: integer (nullable = true)\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- Categoria: string (nullable = true)\n",
      " |-- Contacto_principal: string (nullable = true)\n",
      " |-- Dias_pago: integer (nullable = true)\n",
      " |-- Codigo_postal: integer (nullable = true)\n",
      "\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMACION\n",
    "\n",
    "proveedores_df= proveedores_df.join(cat_proveedores_df, how='left', on ='CategoriaProveedorID')\n",
    "proveedores_df= proveedores_df.join(personas_df, how='left', on = proveedores_df.PersonaContactoPrincipalID == personas_df.ID_persona)\n",
    "proveedores_df= proveedores_df.select('ID_Proveedor_T','Nombre', 'Categoria','Contacto_principal','Dias_pago','Codigo_postal')\n",
    "proveedores_df.printSchema()\n",
    "\n",
    "proveedores_df.head(3)\n",
    "\n",
    "print(proveedores_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(ID_Proveedor_T=0, Nombre='Missing', Categoria='Missing', Contacto_principal='Missing', Dias_pago=None, Codigo_postal=None), Row(ID_Proveedor_T=4, Nombre='Fabrikam, Inc.', Categoria='ropa', Contacto_principal='Bill Lawson', Dias_pago=30, Codigo_postal=40351), Row(ID_Proveedor_T=5, Nombre='Graphic Design Institute', Categoria='productos novedosos', Contacto_principal='Penny Buck', Dias_pago=14, Codigo_postal=64847)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, TimestampType,IntegerType,StringType,DecimalType\n",
    "# Crea el registro para proveedor = nulo\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ID_Proveedor_T\", IntegerType(), nullable=True),\n",
    "    StructField(\"Nombre\", StringType(), nullable=True),\n",
    "    StructField(\"Categoria\", StringType(), nullable=True),\n",
    "    StructField(\"Contacto_principal\", StringType(), nullable=True),\n",
    "    StructField(\"Dias_pago\", IntegerType(), nullable=True),\n",
    "    StructField(\"Codigo_postal\", IntegerType(), nullable=True)\n",
    "    \n",
    "])\n",
    "data = [(0, 'Missing', 'Missing','Missing',None,None)]\n",
    "proveedor_nulo_df = spark.createDataFrame(data, schema)\n",
    "proveedores_df = proveedor_nulo_df.union(proveedores_df)\n",
    "print(proveedores_df.head(3))\n",
    "proveedores_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuvVgJ4R86v0"
   },
   "source": [
    "#### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "id": "yCC2zZqY86v0"
   },
   "outputs": [],
   "source": [
    "# CARGUE\n",
    "guardar_db(dest_db_connection_string, proveedores_df,'Estudiante_111_202315.Proveedor', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+-------------------+--------------------+------------------+---------+-------------+\n",
      "|ID_Proveedor_DWH|ID_Proveedor_T|             Nombre|           Categoria|Contacto_principal|Dias_pago|Codigo_postal|\n",
      "+----------------+--------------+-------------------+--------------------+------------------+---------+-------------+\n",
      "|              45|             6|Humongous Insurance|servicios de seguros|Madelaine  Cartier|      -14|        37770|\n",
      "|              46|             4|     Fabrikam, Inc.|                ropa|       Bill Lawson|       30|        40351|\n",
      "|              47|            12|  The Phone Company| productos novedosos|           Hai Dam|       30|        56732|\n",
      "|              48|            11|      Trey Research|servicios de mark...|      Donald Jones|       -7|        57543|\n",
      "|              49|             9|     Nod Publishers| productos novedosos|      Marcos Costa|        7|        27906|\n",
      "+----------------+--------------+-------------------+--------------------+------------------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Numero de registros 14\n"
     ]
    }
   ],
   "source": [
    "#guardar_db(dest_db_connection_string, proveedores_df,'Estudiante_111_202315.Proveedor', db_user, db_psswd)\n",
    "test = obtener_dataframe_de_bd(dest_db_connection_string, \"Estudiante_111_202315.Proveedor\", db_user, db_psswd)\n",
    "test.show(5)\n",
    "print(f'Numero de registros {test.count()}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Fo-J97586v0"
   },
   "source": [
    "Verifique los resultados usando MySQL Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HpgwA6t86v0"
   },
   "source": [
    "### Dimension :CLIENTE\n",
    "Su fuente de datos es una combinación de las tablas transaccionales <i>CategoriasCliente, GruposCompra y Clientes</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZjDeVYd86v1"
   },
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "id": "2HTzU96W86v1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID_Categoria: integer (nullable = true)\n",
      " |-- NombreCategoria: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ID_GrupoCompra: integer (nullable = true)\n",
      " |-- NombreGrupoCompra: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ID_Cliente_T: integer (nullable = true)\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- ClienteFactura: integer (nullable = true)\n",
      " |-- ID_Categoria: integer (nullable = true)\n",
      " |-- ID_GrupoCompra: integer (nullable = true)\n",
      " |-- ID_CiudadEntrega_DWH: integer (nullable = true)\n",
      " |-- LimiteCredito: decimal(10,0) (nullable = true)\n",
      " |-- FechaAperturaCuenta: timestamp (nullable = true)\n",
      " |-- DiasPago: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#EXTRACCION\n",
    "\n",
    "sql_categoriasCliente = '''(SELECT DISTINCT ID_Categoria, NombreCategoria FROM WWImportersTransactional.CategoriasCliente) AS Temp_categoriasclientes'''\n",
    "sql_gruposCompra = '''(SELECT DISTINCT ID_GrupoCompra, NombreGrupoCompra FROM WWImportersTransactional.GruposCompra) AS Temp_gruposcompra'''\n",
    "sql_clientes = '''(SELECT DISTINCT ID_Cliente as ID_Cliente_T, Nombre, ClienteFactura, ID_Categoria, ID_GrupoCompra, ID_CiudadEntrega AS ID_CiudadEntrega_DWH, LimiteCredito, FechaAperturaCuenta, DiasPago FROM WWImportersTransactional.Clientes) AS Temp_clientes'''\n",
    "\n",
    "categoriasCliente_df = obtener_dataframe_de_bd(source_db_connection_string, sql_categoriasCliente, db_user, db_psswd)\n",
    "gruposCompra_df = obtener_dataframe_de_bd(source_db_connection_string, sql_gruposCompra, db_user, db_psswd)\n",
    "clientes_df = obtener_dataframe_de_bd(source_db_connection_string, sql_clientes, db_user, db_psswd)\n",
    "categoriasCliente_df.printSchema()\n",
    "gruposCompra_df.printSchema()\n",
    "clientes_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7xbgfCk86v1"
   },
   "source": [
    "#### Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "id": "97vmyl3Q86v1",
    "outputId": "0ffd3364-9b73-4001-e9bf-de53c97a2b5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID_Cliente_T: integer (nullable = true)\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- NombreCategoria: string (nullable = false)\n",
      " |-- NombreGrupoCompra: string (nullable = false)\n",
      " |-- ClienteFactura: integer (nullable = true)\n",
      " |-- ID_CiudadEntrega_DWH: integer (nullable = true)\n",
      " |-- LimiteCredito: decimal(10,0) (nullable = true)\n",
      " |-- FechaAperturaCuenta: timestamp (nullable = true)\n",
      " |-- DiasPago: integer (nullable = true)\n",
      "\n",
      "663\n"
     ]
    }
   ],
   "source": [
    "clientes_df = clientes_df.join(gruposCompra_df, how = 'left', on = 'ID_GrupoCompra')\n",
    "clientes_df = clientes_df.join(categoriasCliente_df, how = 'left', on = 'ID_Categoria') \n",
    "clientes_df = clientes_df.select('ID_Cliente_T','Nombre','NombreCategoria','NombreGrupoCompra','ClienteFactura',\n",
    "                          'ID_CiudadEntrega_DWH','LimiteCredito','FechaAperturaCuenta','DiasPago')\n",
    "clientes_df = clientes_df.fillna({'NombreCategoria':'Missing','NombreGrupoCompra':'Missing'})\n",
    "clientes_df.head(5)\n",
    "clientes_df.printSchema()\n",
    "print(clientes_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(ID_Cliente_T=0, Nombre='Missing', NombreCategoria='Missing', NombreGrupoCompra='Missing', ClienteFactura=None, ID_CiudadEntrega_DWH=None, LimiteCredito=None, FechaAperturaCuenta=None, DiasPago=None), Row(ID_Cliente_T=1, Nombre='Tailspin Toys (Head Office)', NombreCategoria='Novelty Shop', NombreGrupoCompra='Tailspin Toys', ClienteFactura=1, ID_CiudadEntrega_DWH=19586, LimiteCredito=None, FechaAperturaCuenta=datetime.datetime(2013, 1, 1, 0, 0), DiasPago=7), Row(ID_Cliente_T=2, Nombre='Tailspin Toys (Sylvanite, MT)', NombreCategoria='Novelty Shop', NombreGrupoCompra='Tailspin Toys', ClienteFactura=1, ID_CiudadEntrega_DWH=33475, LimiteCredito=None, FechaAperturaCuenta=datetime.datetime(2013, 1, 1, 0, 0), DiasPago=7)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "664"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adicionar Cliente con ID =0\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType,IntegerType,StringType,DecimalType\n",
    "# Crea el registro para el id = 0\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ID_Cliente_T\", IntegerType(), nullable=True),\n",
    "    StructField(\"Nombre\", StringType(), nullable=True),\n",
    "    StructField(\"NombreCategoria\", StringType(), nullable=True),\n",
    "    StructField(\"NombreGrupoCompra\", StringType(), nullable=True),\n",
    "    StructField(\"ClienteFactura\", IntegerType(), nullable=True),\n",
    "    StructField(\"ID_CiudadEntrega_DWH\", IntegerType(), nullable=True),\n",
    "    StructField(\"LimiteCredito\", DecimalType(10, 0), nullable=True),\n",
    "    StructField(\"FechaAperturaCuenta\", TimestampType(), nullable=True),\n",
    "    StructField(\"DiasPago\", IntegerType(), nullable=True)\n",
    "])\n",
    "data = [(0, 'Missing', 'Missing','Missing',None,None, None, None, None)]\n",
    "cliente_0_df = spark.createDataFrame(data, schema)\n",
    "clientes_df = cliente_0_df.union(clientes_df)\n",
    "print(clientes_df.head(3))\n",
    "clientes_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rh_102Yy86v1"
   },
   "source": [
    "#### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "id": "UpbxgpWn86v1"
   },
   "outputs": [],
   "source": [
    "# CARGUE\n",
    "guardar_db(dest_db_connection_string, clientes_df,'Estudiante_111_202315.Cliente', db_user, db_psswd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(ID_Cliente_DWH=1334, ID_Cliente_T=804, Nombre='Aleksandrs Riekstins', ClienteFactura=804, ID_CiudadEntrega_DWH=18069, LimiteCredito=Decimal('2200.00'), FechaAperturaCuenta=datetime.date(2013, 1, 1), DiasPago=7, NombreGrupoCompra='Missing', NombreCategoria='Computer Store'), Row(ID_Cliente_DWH=1335, ID_Cliente_T=808, Nombre='Jackson Kolios', ClienteFactura=808, ID_CiudadEntrega_DWH=28221, LimiteCredito=Decimal('1800.00'), FechaAperturaCuenta=datetime.date(2013, 1, 1), DiasPago=7, NombreGrupoCompra='Missing', NombreCategoria='Computer Store'), Row(ID_Cliente_DWH=1336, ID_Cliente_T=809, Nombre='Madhu Dwivedi', ClienteFactura=809, ID_CiudadEntrega_DWH=26105, LimiteCredito=Decimal('1700.00'), FechaAperturaCuenta=datetime.date(2013, 1, 1), DiasPago=7, NombreGrupoCompra='Missing', NombreCategoria='Computer Store')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "664"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#guardar_db(dest_db_connection_string, proveedores_df,'Estudiante_111_202315.Proveedor', db_user, db_psswd)\n",
    "test = obtener_dataframe_de_bd(dest_db_connection_string, \"Estudiante_111_202315.Cliente\", db_user, db_psswd)\n",
    "print(test.head(3))\n",
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTy1oE3V86v2"
   },
   "source": [
    "Verifique los resultados usando MySQL Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYetZ2Ip86v2"
   },
   "source": [
    "### Dimension: PRODUCTOS\n",
    "Su fuente de datos es la combinación entre las tablas transaccionales *Productos y Colores*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HogvCqW_86v2"
   },
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "ZEJjQslF86v2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID_Producto_T: integer (nullable = true)\n",
      " |-- ID_Color: integer (nullable = true)\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- Marca: string (nullable = true)\n",
      " |-- Necesitarefrigeracion: integer (nullable = true)\n",
      " |-- Dias_tiempo_entrega: integer (nullable = true)\n",
      " |-- Precio_minorista_recomendado: decimal(10,0) (nullable = true)\n",
      " |-- Impuesto: decimal(10,0) (nullable = true)\n",
      " |-- Precio_unitario: decimal(10,0) (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ID_Color: integer (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_productos = '''(SELECT DISTINCT ID_Producto as ID_Producto_T, ID_Color, NombreProducto AS Nombre, Marca, Necesita_refrigeracion AS Necesitarefrigeracion, Dias_tiempo_entrega,PrecioRecomendado AS Precio_minorista_recomendado, Impuesto, PrecioUnitario AS Precio_unitario FROM WWImportersTransactional.Producto) AS Temp_productos'''\n",
    "sql_colores = '''(SELECT DISTINCT ID_Color, Color FROM WWImportersTransactional.Colores) AS Temp_colores'''\n",
    "\n",
    "productos_df = obtener_dataframe_de_bd(source_db_connection_string, sql_productos, db_user, db_psswd)\n",
    "colores_df = obtener_dataframe_de_bd(source_db_connection_string, sql_colores, db_user, db_psswd)\n",
    "productos_df.printSchema()\n",
    "colores_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rKmT9jd86v2"
   },
   "source": [
    "#### Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "petU8G6K86v2",
    "outputId": "34d83d0f-f557-4ded-b71e-52afb08dfaf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID_Producto_T: integer (nullable = true)\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- Marca: string (nullable = true)\n",
      " |-- Color: string (nullable = false)\n",
      " |-- Necesitarefrigeracion: integer (nullable = true)\n",
      " |-- Dias_tiempo_entrega: integer (nullable = true)\n",
      " |-- cantidad_por_salida: integer (nullable = true)\n",
      " |-- Precio_minorista_recomendado: decimal(10,0) (nullable = true)\n",
      " |-- Impuesto: decimal(10,0) (nullable = true)\n",
      " |-- Precio_unitario: decimal(10,0) (nullable = true)\n",
      "\n",
      "[Row(ID_Producto_T=1, Nombre='USB missile launcher (Green)', Marca=None, Color='Missing', Necesitarefrigeracion=0, Dias_tiempo_entrega=14, cantidad_por_salida=None, Precio_minorista_recomendado=Decimal('37'), Impuesto=Decimal('15'), Precio_unitario=Decimal('25')), Row(ID_Producto_T=4, Nombre='USB food flash drive - sushi roll', Marca=None, Color='Missing', Necesitarefrigeracion=0, Dias_tiempo_entrega=14, cantidad_por_salida=None, Precio_minorista_recomendado=Decimal('48'), Impuesto=Decimal('15'), Precio_unitario=Decimal('32')), Row(ID_Producto_T=5, Nombre='USB food flash drive - hamburger', Marca=None, Color='Missing', Necesitarefrigeracion=0, Dias_tiempo_entrega=14, cantidad_por_salida=None, Precio_minorista_recomendado=Decimal('48'), Impuesto=Decimal('15'), Precio_unitario=Decimal('32')), Row(ID_Producto_T=3, Nombre='Office cube periscope (Black)', Marca=None, Color='Black', Necesitarefrigeracion=0, Dias_tiempo_entrega=14, cantidad_por_salida=None, Precio_minorista_recomendado=Decimal('28'), Impuesto=Decimal('15'), Precio_unitario=Decimal('19')), Row(ID_Producto_T=2, Nombre='USB rocket launcher (Gray)', Marca=None, Color='Steel Gray', Necesitarefrigeracion=0, Dias_tiempo_entrega=14, cantidad_por_salida=None, Precio_minorista_recomendado=Decimal('37'), Impuesto=Decimal('15'), Precio_unitario=Decimal('25'))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRANSFORMACION\n",
    "\n",
    "productos_df = productos_df.withColumn(\"cantidad_por_salida\", lit(None).cast(\"int\"))\n",
    "productos_df = productos_df.join(colores_df, how = 'left', on = 'ID_Color').fillna({'Color': 'Missing'})\n",
    "#productos = productos.coalesce(1).withColumn('ID_Producto_DWH', f.monotonically_increasing_id() + 1)\n",
    "productos_df = productos_df.select('ID_Producto_T','Nombre','Marca','Color','Necesitarefrigeracion','Dias_tiempo_entrega', 'cantidad_por_salida','Precio_minorista_recomendado','Impuesto','Precio_unitario')\n",
    "productos_df.printSchema()\n",
    "print(productos_df.head(5))\n",
    "\n",
    "productos_df.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9SnkMUH86v3"
   },
   "source": [
    "#### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "MMArotXz86v3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID_Producto_T: int, Nombre: string, Marca: string, Color: string, Necesitarefrigeracion: int, Dias_tiempo_entrega: int, cantidad_por_salida: int, Precio_minorista_recomendado: decimal(10,0), Impuesto: decimal(10,0), Precio_unitario: decimal(10,0)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CARGUE\n",
    "guardar_db(dest_db_connection_string, productos_df,'Estudiante_111_202315.Producto', db_user, db_psswd)\n",
    "productos_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(ID_Producto_DWH=6, ID_Producto_T=1, Nombre='USB missile launcher (Green)', Marca=None, Color='Missing', Necesitarefrigeracion=False, Dias_tiempo_entrega=14, cantidad_por_salida=None, Precio_minorista_recomendado=Decimal('37.00'), Impuesto=Decimal('15.00'), Precio_unitario=Decimal('25')), Row(ID_Producto_DWH=7, ID_Producto_T=4, Nombre='USB food flash drive - sushi roll', Marca=None, Color='Missing', Necesitarefrigeracion=False, Dias_tiempo_entrega=14, cantidad_por_salida=None, Precio_minorista_recomendado=Decimal('48.00'), Impuesto=Decimal('15.00'), Precio_unitario=Decimal('32')), Row(ID_Producto_DWH=8, ID_Producto_T=5, Nombre='USB food flash drive - hamburger', Marca=None, Color='Missing', Necesitarefrigeracion=False, Dias_tiempo_entrega=14, cantidad_por_salida=None, Precio_minorista_recomendado=Decimal('48.00'), Impuesto=Decimal('15.00'), Precio_unitario=Decimal('32'))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "454"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#guardar_db(dest_db_connection_string, productos_df,'Estudiante_111_202315.Producto', db_user, db_psswd)\n",
    "test = obtener_dataframe_de_bd(dest_db_connection_string, \"Estudiante_111_202315.Producto\", db_user, db_psswd)\n",
    "print(test.head(3))\n",
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YglraEoA86v3"
   },
   "source": [
    "Verifique los resultados usando MySQL Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UM4L4jA086v3"
   },
   "source": [
    "### Dimension FECHA\n",
    "Su fuente de datos son las fechas encontradas en MovimientosCopia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8O1GvOd86v3"
   },
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "id": "znROuHS086v3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TransaccionProductoID: integer (nullable = true)\n",
      " |-- ProductoID: integer (nullable = true)\n",
      " |-- TipoTransaccionID: integer (nullable = true)\n",
      " |-- ClienteID: double (nullable = true)\n",
      " |-- InvoiceID: double (nullable = true)\n",
      " |-- ProveedorID: string (nullable = true)\n",
      " |-- OrdenDeCompraID: string (nullable = true)\n",
      " |-- FechaTransaccion: string (nullable = true)\n",
      " |-- Cantidad: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "236667"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sql_movimientos = '''(SELECT DISTINCT * FROM WWImportersTransactional.movimientos) AS Temp_movimientos'''\n",
    "movimientos_df = obtener_dataframe_de_bd(db_connection_string, sql_movimientos, db_user, db_psswd)\n",
    "movimientos_df.printSchema()\n",
    "movimientos_df.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mo633Vpg86v3"
   },
   "source": [
    "#### Transformación\n",
    "\n",
    "T1. Estandarizar la fecha. Se evidencia dos tipos de formato de fecha \n",
    "- YYYY-MM-DD HH:mm:SS.000000\n",
    "- MMM dd, YYYY\n",
    "\n",
    "T2. Convertir fecha estandarizado a integer para serel Key de esta dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "id": "wwK8bV8Z86v3",
    "outputId": "7b494261-1640-4108-98f8-65fbe04c7b91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172413\n",
      "root\n",
      " |-- TransaccionProductoID: integer (nullable = true)\n",
      " |-- ProductoID: integer (nullable = true)\n",
      " |-- TipoTransaccionID: integer (nullable = true)\n",
      " |-- ClienteID: double (nullable = true)\n",
      " |-- InvoiceID: double (nullable = true)\n",
      " |-- ProveedorID: string (nullable = true)\n",
      " |-- OrdenDeCompraID: string (nullable = true)\n",
      " |-- FechaTransaccion: string (nullable = true)\n",
      " |-- Cantidad: double (nullable = true)\n",
      " |-- fecha_estandar: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "236667"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRANSFORMACION \n",
    "#T1.\n",
    "\n",
    "regex = \"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\"\n",
    "cumpleformato_df = movimientos_df.filter(movimientos_df[\"FechaTransaccion\"].rlike(regex))\n",
    "print(cumpleformato_df.count())\n",
    "\n",
    "def estandarizar_fecha(fecha):\n",
    "    regex= \"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\"\n",
    "    match = re.match(regex, fecha)\n",
    "    if match:\n",
    "        nueva_fecha= fecha[0:19]\n",
    "        return datetime.strptime(nueva_fecha, \"%Y-%m-%d %H:%M:%S\")\n",
    "    else:\n",
    "        regex = \"^(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s[0-3]?[0-9],[0-9]{4}$\"\n",
    "        match = re.match(regex, fecha)\n",
    "        if match:\n",
    "            return datetime.strptime(fecha, \"%b %d,%Y\")\n",
    "        else:\n",
    "            return None;\n",
    "\n",
    "func = udf(estandarizar_fecha, DateType())\n",
    "movimientos_df=movimientos_df.withColumn('fecha_estandar',func(col('FechaTransaccion')))\n",
    "movimientos_df.printSchema()\n",
    "movimientos_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID_Fecha: integer (nullable = true)\n",
      " |-- Fecha: date (nullable = true)\n",
      " |-- Dia: integer (nullable = true)\n",
      " |-- Mes: integer (nullable = true)\n",
      " |-- Anyo: integer (nullable = true)\n",
      " |-- Numero_Semana_ISO: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(ID_Fecha=20150519, Fecha=datetime.date(2015, 5, 19), Dia=19, Mes=5, Anyo=2015, Numero_Semana_ISO=21),\n",
       " Row(ID_Fecha=20160301, Fecha=datetime.date(2016, 3, 1), Dia=1, Mes=3, Anyo=2016, Numero_Semana_ISO=9)]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T2.\n",
    "fechas_df =movimientos_df.selectExpr('fecha_estandar as Fecha').distinct()\n",
    "fechas_df = fechas_df.selectExpr('Fecha', \"day(Fecha) as Dia\", \"month(Fecha) as Mes\",\"year(Fecha) as Anyo\",\"weekofyear(Fecha) as Numero_Semana_ISO\" )\n",
    "fechas_df = fechas_df.withColumn(\"ID_Fecha\", date_format(col(\"Fecha\"), \"yyyyMMdd\"))\n",
    "fechas_df = fechas_df.withColumn(\"ID_Fecha\", col(\"ID_Fecha\").cast('int'))\n",
    "fechas_df = fechas_df.select(col('ID_Fecha'),col('Fecha'), col('Dia'),col('Mes'), col('Anyo'), col('Numero_Semana_ISO'))\n",
    "fechas_df.printSchema()\n",
    "fechas_df.head(2)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i36NawhX86v4"
   },
   "source": [
    "#### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "id": "QcvMzmCf86v4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID_Fecha: int, Fecha: date, Dia: int, Mes: int, Anyo: int, Numero_Semana_ISO: int]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CARGUE\n",
    "\n",
    "guardar_db(dest_db_connection_string, fechas_df,'Estudiante_111_202315.Fecha', db_user, db_psswd)\n",
    "fechas_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---+---+----+-----------------+\n",
      "|ID_Fecha|     Fecha|Dia|Mes|Anyo|Numero_Semana_ISO|\n",
      "+--------+----------+---+---+----+-----------------+\n",
      "|20130101|2013-01-01|  1|  1|2013|                1|\n",
      "|20130102|2013-01-02|  2|  1|2013|                1|\n",
      "|20130103|2013-01-03|  3|  1|2013|                1|\n",
      "|20130104|2013-01-04|  4|  1|2013|                1|\n",
      "|20130105|2013-01-05|  5|  1|2013|                1|\n",
      "+--------+----------+---+---+----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Numero de registros 1070\n"
     ]
    }
   ],
   "source": [
    "#guardar_db(dest_db_connection_string, fechas_df,'Estudiante_111_202315.Fecha', db_user, db_psswd)#\n",
    "#fechas_df.unpersist()\n",
    "test = obtener_dataframe_de_bd(dest_db_connection_string, \"Estudiante_111_202315.Fecha\", db_user, db_psswd)\n",
    "test.show(5)\n",
    "print(f'Numero de registros {test.count()}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMyvdEB-86v4"
   },
   "source": [
    "Verifique los resultados usando MySQL Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ocmdLWv86v4"
   },
   "source": [
    "### Hechos: HECHO_MOVIMIENTO\n",
    "Su fuente de datos es la combinación entre las tablas transaccionales Ordenes y detalles de orden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvgnsPfK86v4"
   },
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "id": "9b4Qs9_886v4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TransaccionProductoID: integer (nullable = true)\n",
      " |-- ProductoID: integer (nullable = true)\n",
      " |-- TipoTransaccionID: integer (nullable = true)\n",
      " |-- ClienteID: double (nullable = true)\n",
      " |-- InvoiceID: double (nullable = true)\n",
      " |-- ProveedorID: string (nullable = true)\n",
      " |-- OrdenDeCompraID: string (nullable = true)\n",
      " |-- FechaTransaccion: string (nullable = true)\n",
      " |-- Cantidad: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ID_Producto_DWH: integer (nullable = true)\n",
      " |-- ID_Producto_T: integer (nullable = true)\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- Marca: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Necesitarefrigeracion: boolean (nullable = true)\n",
      " |-- Dias_tiempo_entrega: integer (nullable = true)\n",
      " |-- cantidad_por_salida: integer (nullable = true)\n",
      " |-- Precio_minorista_recomendado: decimal(10,2) (nullable = true)\n",
      " |-- Impuesto: decimal(10,2) (nullable = true)\n",
      " |-- Precio_unitario: decimal(10,0) (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ID_Proveedor_DWH: integer (nullable = true)\n",
      " |-- ID_Proveedor_T: integer (nullable = true)\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- Categoria: string (nullable = true)\n",
      " |-- Contacto_principal: string (nullable = true)\n",
      " |-- Dias_pago: integer (nullable = true)\n",
      " |-- Codigo_postal: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ID_Cliente_DWH: integer (nullable = true)\n",
      " |-- ID_Cliente_T: integer (nullable = true)\n",
      " |-- Nombre: string (nullable = true)\n",
      " |-- ClienteFactura: integer (nullable = true)\n",
      " |-- ID_CiudadEntrega_DWH: integer (nullable = true)\n",
      " |-- LimiteCredito: decimal(10,2) (nullable = true)\n",
      " |-- FechaAperturaCuenta: date (nullable = true)\n",
      " |-- DiasPago: integer (nullable = true)\n",
      " |-- NombreGrupoCompra: string (nullable = true)\n",
      " |-- NombreCategoria: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ID_Tipo_transaccion_DWH: integer (nullable = true)\n",
      " |-- ID_Tipo_transaccion_T: integer (nullable = true)\n",
      " |-- Tipo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sql_ordenes = '''(SELECT DISTINCT * FROM WWImportersTransactional.Ordenes) AS Temp_ordenes'''\n",
    "#sql_detallesOrdenes = '''(SELECT DISTINCT * FROM WWImportersTransactional.DetallesOrdenes) AS Temp_detallesordenes'''\n",
    "#ordenes = obtener_dataframe_de_bd(source_db_connection_string, sql_ordenes, db_user, db_psswd)\n",
    "#detallesOrdenes = obtener_dataframe_de_bd(source_db_connection_string, sql_detallesOrdenes, db_user, db_psswd)\n",
    "\n",
    "sql_movimientos = '''(SELECT DISTINCT * FROM WWImportersTransactional.movimientos) AS Temp_movimientos'''\n",
    "sql_productos = '''(SELECT DISTINCT * FROM Estudiante_111_202315.Producto) AS Temp_producto'''\n",
    "sql_proveedores =  '''(SELECT DISTINCT * FROM Estudiante_111_202315.Proveedor) AS Temp_proveedor'''\n",
    "sql_clientes =  '''(SELECT DISTINCT * FROM Estudiante_111_202315.Cliente) AS Temp_cliente'''\n",
    "sql_tipostrans= '''(SELECT DISTINCT * FROM Estudiante_111_202315.TipoTransaccion) AS Temp_tipotran'''\n",
    "\n",
    "#sql_detallesOrdenes = '''(SELECT DISTINCT * FROM WWImportersTransactional.DetallesOrdenes) AS Temp_detallesordenes'''\n",
    "movimientos_df = obtener_dataframe_de_bd(source_db_connection_string, sql_movimientos, db_user, db_psswd)\n",
    "movimientos_df.printSchema()\n",
    "productos_dwh_df= obtener_dataframe_de_bd(dest_db_connection_string, sql_productos, db_user, db_psswd)\n",
    "productos_dwh_df.printSchema()\n",
    "proveedores_dwh_df= obtener_dataframe_de_bd(dest_db_connection_string, sql_proveedores, db_user, db_psswd)\n",
    "proveedores_dwh_df.printSchema()\n",
    "clientes_dwh_df= obtener_dataframe_de_bd(dest_db_connection_string, sql_clientes, db_user, db_psswd)\n",
    "clientes_dwh_df.printSchema()\n",
    "tipotran_dwh_df= obtener_dataframe_de_bd(dest_db_connection_string, sql_tipostrans, db_user, db_psswd)\n",
    "tipotran_dwh_df.printSchema()\n",
    "\n",
    "\n",
    "#detallesOrdenes = obtener_dataframe_de_bd(source_db_connection_string, sql_detallesOrdenes, db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------+-----------------+---------+---------+-----------+---------------+----------------+--------+-----+\n",
      "|TransaccionProductoID|ProductoID|TipoTransaccionID|ClienteID|InvoiceID|ProveedorID|OrdenDeCompraID|FechaTransaccion|Cantidad|count|\n",
      "+---------------------+----------+-----------------+---------+---------+-----------+---------------+----------------+--------+-----+\n",
      "+---------------------+----------+-----------------+---------+---------+-----------+---------------+----------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# VErficar no duplicados\n",
    "grupo_df = movimientos_df.groupBy(movimientos_df.columns)\n",
    "cuentagrupos_df = grupo_df.count()\n",
    "#Obtener si hay duplicados\n",
    "cuentagrupos_df.filter(cuentagrupos_df[\"count\"] > 1).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TransaccionProductoID: integer (nullable = true)\n",
      " |-- ID_Producto_T: integer (nullable = true)\n",
      " |-- ID_Tipo_transaccion_T: integer (nullable = true)\n",
      " |-- ID_Cliente_T: integer (nullable = true)\n",
      " |-- ID_Proveedor_T: integer (nullable = true)\n",
      " |-- Fecha: date (nullable = true)\n",
      " |-- Dia: integer (nullable = true)\n",
      " |-- Mes: integer (nullable = true)\n",
      " |-- Anyo: integer (nullable = true)\n",
      " |-- Cantidad: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movimientos_df = movimientos_df.withColumn(\"ProveedorID\", when(col(\"ProveedorID\") == \"\", 0).otherwise(col('ProveedorID').cast(\"int\")))\n",
    "movimientos_df = movimientos_df.withColumn(\"ClienteID\", when(col(\"ClienteID\").isNull(), 0).otherwise(col('ClienteID').cast(\"int\")))\n",
    "movimientos_df=movimientos_df.withColumn('Fecha',func(col('FechaTransaccion')))\n",
    "movimientos_df= movimientos_df.selectExpr('TransaccionProductoID','ProductoID as ID_Producto_T','TipoTransaccionID as ID_Tipo_transaccion_T', 'ClienteID as ID_Cliente_T','ProveedorID as ID_Proveedor_T', 'Fecha',\"day(Fecha) as Dia\", \"month(Fecha) as Mes\",\"year(Fecha) as Anyo\",\"Cantidad\" )\n",
    "movimientos_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID_Fecha: integer (nullable = true)\n",
      " |-- TransaccionProductoID: integer (nullable = true)\n",
      " |-- ID_Producto_T: integer (nullable = true)\n",
      " |-- ID_Proveedor_T: integer (nullable = true)\n",
      " |-- ID_Cliente_T: integer (nullable = true)\n",
      " |-- ID_Tipo_transaccion_T: integer (nullable = true)\n",
      " |-- Cantidad: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movimientos_df = movimientos_df.withColumn(\"ID_Fecha\", date_format(col(\"Fecha\"), \"yyyyMMdd\"))\n",
    "movimientos_df = movimientos_df.withColumn(\"ID_Fecha\", col(\"ID_Fecha\").cast('int'))\n",
    "movimientos_df= movimientos_df.select(col('ID_Fecha'), col('TransaccionProductoID'),col('ID_Producto_T'),col('ID_Proveedor_T'),col('ID_Cliente_T'), col('ID_Tipo_transaccion_T'), col('Cantidad'))\n",
    "movimientos_df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID_Fecha: integer (nullable = true)\n",
      " |-- ID_Producto_DWH: integer (nullable = true)\n",
      " |-- ID_Proveedor_T: integer (nullable = true)\n",
      " |-- ID_Cliente_T: integer (nullable = true)\n",
      " |-- ID_Tipo_transaccion_T: integer (nullable = true)\n",
      " |-- Cantidad: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ID_Fecha: integer (nullable = true)\n",
      " |-- ID_Producto_DWH: integer (nullable = true)\n",
      " |-- ID_Proveedor_DWH: integer (nullable = true)\n",
      " |-- ID_Cliente_T: integer (nullable = true)\n",
      " |-- ID_Tipo_transaccion_T: integer (nullable = true)\n",
      " |-- Cantidad: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ID_Fecha: integer (nullable = true)\n",
      " |-- ID_Producto_DWH: integer (nullable = true)\n",
      " |-- ID_Proveedor_DWH: integer (nullable = true)\n",
      " |-- ID_Cliente_DWH: integer (nullable = true)\n",
      " |-- ID_Tipo_transaccion_T: integer (nullable = true)\n",
      " |-- Cantidad: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ID_Fecha: integer (nullable = true)\n",
      " |-- ID_Producto_DWH: integer (nullable = true)\n",
      " |-- ID_Proveedor_DWH: integer (nullable = true)\n",
      " |-- ID_Cliente_DWH: integer (nullable = true)\n",
      " |-- ID_Tipo_transaccion_DWH: integer (nullable = true)\n",
      " |-- Cantidad: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "473334"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "temp_df = movimientos_df.join(productos_dwh_df, how = 'left', on = 'ID_Producto_T')\n",
    "temp_df= temp_df.select(col('ID_Fecha'), col('ID_Producto_DWH'),col('ID_Proveedor_T'),col('ID_Cliente_T'), col('ID_Tipo_transaccion_T'), col('Cantidad'))\n",
    "temp_df.printSchema()\n",
    "\n",
    "temp_df = temp_df.join(proveedores_dwh_df, how = 'left', on = 'ID_Proveedor_T')\n",
    "temp_df= temp_df.select(col('ID_Fecha'), col('ID_Producto_DWH'),col('ID_Proveedor_DWH'),col('ID_Cliente_T'), col('ID_Tipo_transaccion_T'), col('Cantidad'))\n",
    "temp_df.printSchema()\n",
    "\n",
    "temp_df = temp_df.join(clientes_dwh_df, how = 'left', on = 'ID_Cliente_T')\n",
    "temp_df= temp_df.select(col('ID_Fecha'), col('ID_Producto_DWH'),col('ID_Proveedor_DWH'),col('ID_Cliente_DWH'), col('ID_Tipo_transaccion_T'), col('Cantidad'))\n",
    "temp_df.printSchema()\n",
    "\n",
    "\n",
    "temp_df = temp_df.join(tipotran_dwh_df, how = 'left', on = 'ID_Tipo_transaccion_T')\n",
    "temp_df= temp_df.select(col('ID_Fecha'), col('ID_Producto_DWH'),col('ID_Proveedor_DWH'),col('ID_Cliente_DWH'), col('ID_Tipo_transaccion_DWH'), col('Cantidad'))\n",
    "temp_df.printSchema()\n",
    "\n",
    "temp_df.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#temp_df.show(3)\n",
    "#consistencia: revisar genially: definicion de consistencia\n",
    "ids_ordenes = set([x.ID_Proveedor_T for x in movimientos_df.select('ID_Proveedor_T').collect()])\n",
    "ids_detalles = set([x.ID_Proveedor_T for x in proveedores_dwh_df.select('ID_Proveedor_T').collect()])\n",
    "\n",
    "#len(ids_ordenes-ids_detalles), len(ids_detalles-ids_ordenes)\n",
    "ids_ordenes-ids_detalles\n",
    "#ids_detalles-ids_ordenes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_ordenes = set([x.ID_Cliente_T for x in movimientos_df.select('ID_Cliente_T').collect()])\n",
    "ids_detalles = set([x.ID_Cliente_T for x in clientes_dwh_df.select('ID_Cliente_T').collect()])\n",
    "\n",
    "#len(ids_ordenes-ids_detalles), len(ids_detalles-ids_ordenes)\n",
    "ids_ordenes-ids_detalles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID_Fecha: int, ID_Producto_DWH: int, ID_Proveedor_DWH: int, ID_Cliente_DWH: int, ID_Tipo_transaccion_DWH: int, Cantidad: double]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guardar_db(dest_db_connection_string, temp_df,'Estudiante_111_202315.Hecho_Movimiento', db_user, db_psswd)#\n",
    "temp_df.unpersist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+----------------+--------------+-----------------------+--------+\n",
      "|ID_Fecha|ID_Producto_DWH|ID_Proveedor_DWH|ID_Cliente_DWH|ID_Tipo_transaccion_DWH|Cantidad|\n",
      "+--------+---------------+----------------+--------------+-----------------------+--------+\n",
      "|20140917|            413|              58|          1850|                     14|     -60|\n",
      "|20140917|            186|              58|          1850|                     14|     -60|\n",
      "|20141107|            413|              58|          1401|                     14|     -60|\n",
      "|20141107|            186|              58|          1401|                     14|     -60|\n",
      "|20150714|            413|              58|          1909|                     14|     -60|\n",
      "+--------+---------------+----------------+--------------+-----------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Numero de registros 473334\n"
     ]
    }
   ],
   "source": [
    "#guardar_db(dest_db_connection_string, fechas_df,'Estudiante_111_202315.Fecha', db_user, db_psswd)#\n",
    "#fechas_df.unpersist()\n",
    "test = obtener_dataframe_de_bd(dest_db_connection_string, \"Estudiante_111_202315.Hecho_Movimiento\", db_user, db_psswd)\n",
    "test.show(5)\n",
    "print(f'Numero de registros {test.count()}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTv_CIOT86v5"
   },
   "source": [
    "#### Transformación\n",
    "Estas son las respuestas de Wide World Importers a los conclusiones obtenidas en el entendimiento de los datos:\n",
    "- La regla de negocio \"La tasa de impuesto es de 10% o 15%\" es correcta, pero habian errores en la tabla original, que fueron corregidos. \n",
    "- Para la segunda regla de negocio: \"Son 73.595 órdenes detalladas en 231.412 lineas de detalle de órdenes realizadas desde 2013\", si faltaban datos, los cuales fueron completados, y nos dicen que en cuanto a consistencia ellos revisaron las tablas e hicieron correcciones, pero que los duplicados completos de ordenes los eliminemos\n",
    "- \"El formato de fechas manejado es YYYY-MM-DD HH:MM:SS si tienen hora, minutos y segundos. De lo contrario el formato es YYYY-MM-DD\": En cuanto a formatos de fechas estan de acuerdo con que los estandarizemos y el formato sea el especificado en la regla\n",
    "- Para las descripciones de productos que eran \"a\", se actualizaron a los valores reales. \n",
    "- Se pueden eliminar las columnas Comenarios, Instrucciones_de_entrega y comentarios_internos porque estan vacias. \n",
    "- A pesar de estar en un proceso de mejorar la calidad de los datos y mantener los nulos nos ayudaría a reflejar esa calidad, de la mano con el grupo de analitica de WWI se decide imputar por la media el valor extremo de la variable Cantidad\n",
    "- Para las ordenes las columnas Seleccionado_por_ID_de_persona, ID_de_pedido_pendiente, Seleccion_completada_cuando, y para las columnas Seleccion_completada_cuando de la tabla detalles de ordenes, se decide mantener los valores vacíos, sin embargo para la variable Precio_unitario el negocio reviso y complemento los valores faltantes\n",
    "\n",
    "Las tablas usadas en el tutorial de entendimiento de datos estaran disponibles para su revision con los siguientes nombres: OrdenesCopia y DetallesOrdenesCopia. \n",
    "\n",
    "Para este tutorial vamos a trabajar con unas tablas que dadas las conclusiones del tutorial de entendimiento, WWImporters revisó los datos originales, creo tablas y las llamo \"Ordenes\" y \"DetallesOrdenes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcHdgPzj86v5"
   },
   "source": [
    "Se hace una verificación de los valores de la tasa de impuesto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "GT5i7Yjq86v5",
    "outputId": "95819129-618e-4d35-dd6b-878ea1f88ce8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|Tasa_de_impuesto|\n",
      "+----------------+\n",
      "|              10|\n",
      "|              15|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detallesOrdenes.select(\"Tasa_de_impuesto\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DI27nmvB86v5"
   },
   "source": [
    "Se hace una verificación del rango de fechas disponible en los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "N0GmNF6x86v5",
    "outputId": "326502fa-449d-47e3-faa5-ecd2fb34651b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|min(Fecha_de_pedido)|\n",
      "+--------------------+\n",
      "|          2013-01-01|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordenes.agg({\"Fecha_de_pedido\": \"min\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAp83aSn86v6"
   },
   "source": [
    "Se elimina columnas Comenarios, Instrucciones_de_entrega y comentarios_internos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "UX2Wl4zo86v6"
   },
   "outputs": [],
   "source": [
    "ordenes = ordenes.drop(*[\"Comentarios\", \"Instrucciones_de_entrega\",\"comentarios_internos\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfONrMtq86v6"
   },
   "source": [
    "Se eliminan duplicados totales de ordenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "g3KbTJ4k86v6",
    "outputId": "e0783886-2dde-4a7f-f031-72d442ba87ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93629, 93629)\n"
     ]
    }
   ],
   "source": [
    "print((ordenes.count(),ordenes.distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ocWjOSPD86v6"
   },
   "outputs": [],
   "source": [
    "ordenes = ordenes.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "-qF5L6I186v6",
    "outputId": "e2e4a3b4-025b-41b0-e7da-057df910b082"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93629, 93629)\n"
     ]
    }
   ],
   "source": [
    "print((ordenes.count(),ordenes.distinct().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqSCkt_V86v6"
   },
   "source": [
    "Se hace verificación de consistencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "DXNKhNV986v7",
    "outputId": "2b6014c8-837e-4fc5-f98b-973e8899ac0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#consistencia: revisar genially: definicion de consistencia\n",
    "ids_ordenes = set([x.ID_de_pedido for x in ordenes.select('ID_de_pedido').collect()])\n",
    "ids_detalles = set([x.ID_de_pedido for x in detallesOrdenes.select('ID_de_pedido').collect()])\n",
    "\n",
    "len(ids_ordenes-ids_detalles), len(ids_detalles-ids_ordenes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TB8BrURo86v7"
   },
   "source": [
    "En el siguiente código para el manejo de fechas, pasamos del formato MM dd,YYYY al formato establecido en la regla de negocio<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ajB_7vTr86v7",
    "outputId": "9cabe64a-58b5-472a-9fce-0eeedfcdb0d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20034 73595\n",
      "+------------+-------------+--------------+------------------------------+-------------------------+----------------------+---------------+-------------------------+--------------------------------------+-------------------------------------------+---------------------------+\n",
      "|ID_de_pedido|ID_de_cliente|ID_de_vendedor|Seleccionado_por_ID_de_persona|ID_de_persona_de_contacto|ID_de_pedido_pendiente|Fecha_de_pedido|Fecha_de_entrega_esperada|Numero_de_pedido_de_compra_del_cliente|Pedido_pendiente_de_suministro_insuficiente|Seleccion_completada_cuando|\n",
      "+------------+-------------+--------------+------------------------------+-------------------------+----------------------+---------------+-------------------------+--------------------------------------+-------------------------------------------+---------------------------+\n",
      "|       20972|          132|             6|                             8|                     1263|                  null|    Jan 28,2014|               2014-01-29|                                 14875|                                       true|        2014-01-28 11:00:00|\n",
      "|       22080|          107|            16|                            12|                     1213|                  null|    Feb 17,2014|               2014-02-18|                                 10447|                                       true|        2014-02-17 11:00:00|\n",
      "|       22194|          136|             3|                          null|                     1271|                 22217|    Feb 19,2014|               2014-02-20|                                 12442|                                       true|        2014-02-19 12:00:00|\n",
      "|       22298|          969|             2|                             7|                     3169|                  null|    Feb 20,2014|               2014-02-21|                                 11473|                                       true|        2014-07-10 11:00:00|\n",
      "|       22358|          534|             8|                             6|                     2267|                  null|    Feb 21,2014|               2014-02-24|                                 10078|                                       true|        2014-02-21 11:00:00|\n",
      "+------------+-------------+--------------+------------------------------+-------------------------+----------------------+---------------+-------------------------+--------------------------------------+-------------------------------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20034, 93629)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRANSFORMACION\n",
    "regex = \"([0-2]\\d{3}-(0[1-9]|1[0-2])-(0[1-9]|[1-2][0-9]|3[0-1]))\"\n",
    "cumplenFormato = ordenes.filter(ordenes[\"Fecha_de_pedido\"].rlike(regex))\n",
    "noCumplenFormato = ordenes.filter(~ordenes[\"Fecha_de_pedido\"].rlike(regex))\n",
    "print(noCumplenFormato.count(), cumplenFormato.count())\n",
    "print(noCumplenFormato.show(5))\n",
    "noCumplenFormato = noCumplenFormato.withColumn('Fecha_de_pedido', f.udf(lambda d: datetime.strptime(d, '%b %d,%Y').strftime('%Y-%m-%d'), t.StringType())(f.col('Fecha_de_pedido')))\n",
    "ordenes = noCumplenFormato.union(cumplenFormato)\n",
    "noCumplenFormato.count(), ordenes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JN-fsV5_86v7"
   },
   "source": [
    "Descripciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "8lPrvq2m86v7",
    "outputId": "5eb1f4eb-5b44-4753-d094-ba16ad225fd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+-----------+-----------+---------------+--------+---------------+----------------+---------------------+---------------------------+\n",
      "|Detalle_orden_ID|ID_de_pedido|ID_Producto|Descripcion|ID_Tipo_Paquete|Cantidad|Precio_unitario|Tasa_de_impuesto|Cantidad_seleccionada|Seleccion_completada_cuando|\n",
      "+----------------+------------+-----------+-----------+---------------+--------+---------------+----------------+---------------------+---------------------------+\n",
      "+----------------+------------+-----------+-----------+---------------+--------+---------------+----------------+---------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detallesOrdenes.where(length(col(\"Descripcion\")) <= 10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a31mBQwD86v7"
   },
   "source": [
    "Imputar valor maximo de cantidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "dAeVZ5Xc86v8",
    "outputId": "6a8395e8-cdeb-44b7-a58d-1428b6682823"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Cantidad=360)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detallesOrdenes.select('Cantidad').sort(col(\"Cantidad\").desc()).collect()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "QEM7tcJ986v8"
   },
   "outputs": [],
   "source": [
    "detallesOrdenes = detallesOrdenes.replace( 10000000, 360, 'Cantidad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "mmteyIwh86v8",
    "outputId": "79f8b8d0-b28a-42b4-c9bb-6c3eae3f537f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Cantidad=360)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detallesOrdenes.select('Cantidad').sort(col(\"Cantidad\").desc()).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "uda-rfmr86v8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+-----------+--------------------+---------------+--------+---------------+----------------+---------------------+---------------------------+\n",
      "|Detalle_orden_ID|ID_de_pedido|ID_Producto|         Descripcion|ID_Tipo_Paquete|Cantidad|Precio_unitario|Tasa_de_impuesto|Cantidad_seleccionada|Seleccion_completada_cuando|\n",
      "+----------------+------------+-----------+--------------------+---------------+--------+---------------+----------------+---------------------+---------------------------+\n",
      "|               1|          45|        164|32 mm Double side...|              7|      50|            112|              15|                   50|        2013-01-02 11:00:00|\n",
      "|               2|           1|         67|Ride on toy sedan...|              7|      10|            230|              15|                   10|        2013-01-01 11:00:00|\n",
      "|               3|           2|         50|Developer joke mu...|              7|       9|             13|              15|                    9|        2013-01-01 11:00:00|\n",
      "|               4|          46|         89|\"The Gu\" red shir...|              7|      72|             18|              15|                   72|        2013-01-02 11:00:00|\n",
      "|               5|          46|        171|32 mm Anti static...|              7|      90|             32|              15|                   90|        2013-01-02 11:00:00|\n",
      "+----------------+------------+-----------+--------------------+---------------+--------+---------------+----------------+---------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detallesOrdenes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "QOcOg5wJ86v8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------------+------------------------------+-------------------------+----------------------+---------------+-------------------------+--------------------------------------+-------------------------------------------+---------------------------+\n",
      "|ID_de_pedido|ID_de_cliente|ID_de_vendedor|Seleccionado_por_ID_de_persona|ID_de_persona_de_contacto|ID_de_pedido_pendiente|Fecha_de_pedido|Fecha_de_entrega_esperada|Numero_de_pedido_de_compra_del_cliente|Pedido_pendiente_de_suministro_insuficiente|Seleccion_completada_cuando|\n",
      "+------------+-------------+--------------+------------------------------+-------------------------+----------------------+---------------+-------------------------+--------------------------------------+-------------------------------------------+---------------------------+\n",
      "|       20972|          132|             6|                             8|                     1263|                  null|     2014-01-28|               2014-01-29|                                 14875|                                       true|        2014-01-28 11:00:00|\n",
      "|       22080|          107|            16|                            12|                     1213|                  null|     2014-02-17|               2014-02-18|                                 10447|                                       true|        2014-02-17 11:00:00|\n",
      "|       22194|          136|             3|                          null|                     1271|                 22217|     2014-02-19|               2014-02-20|                                 12442|                                       true|        2014-02-19 12:00:00|\n",
      "|       22298|          969|             2|                             7|                     3169|                  null|     2014-02-20|               2014-02-21|                                 11473|                                       true|        2014-07-10 11:00:00|\n",
      "|       22358|          534|             8|                             6|                     2267|                  null|     2014-02-21|               2014-02-24|                                 10078|                                       true|        2014-02-21 11:00:00|\n",
      "+------------+-------------+--------------+------------------------------+-------------------------+----------------------+---------------+-------------------------+--------------------------------------+-------------------------------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordenes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRyB9egF86v8"
   },
   "source": [
    "Se unen los dos dataframes en un nuevo dataframe, se verifica que no haya duplicados y si los hay se eliminan. Se crea un nuevo dataframe que va a tener toda la información del hecho orden transformada y lista para continuar el proceso de cargue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "3RVZ_KIj86v8",
    "outputId": "90b92db0-5f18-44a1-a14a-73ab6e1ce8f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(294314, 231412)\n",
      "+--------------+-----------+---------------+-------------+--------------+---------------+--------+-----------+---------+\n",
      "|ID_de_pedido_T|ID_Producto|Fecha_de_pedido|ID_de_cliente|ID_de_vendedor|ID_Tipo_Paquete|Cantidad|Valor_total|Impuestos|\n",
      "+--------------+-----------+---------------+-------------+--------------+---------------+--------+-----------+---------+\n",
      "|           148|        203|     2013-01-02|          812|            13|              7|      40|       1280|    19200|\n",
      "|           463|         64|     2013-01-09|          555|             3|              7|       1|         30|      450|\n",
      "|           463|         10|     2013-01-09|          555|             3|              7|       8|        256|     3840|\n",
      "|           463|         16|     2013-01-09|          555|             3|              7|      10|        130|     1950|\n",
      "|           463|         57|     2013-01-09|          555|             3|              7|       3|         39|      585|\n",
      "+--------------+-----------+---------------+-------------+--------------+---------------+--------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordenes_tmp =ordenes\n",
    "ordenes_tmp = ordenes_tmp.join(detallesOrdenes, how = 'inner', on = 'ID_de_pedido')\n",
    "ordenes_tmp = ordenes_tmp.withColumn('Valor_total',col('Precio_unitario')*col('Cantidad'))\n",
    "ordenes_tmp = ordenes_tmp.withColumn('Impuestos',col('Valor_total')*col('Tasa_de_impuesto'))\n",
    "ordenes_tmp = ordenes_tmp.selectExpr('ID_de_pedido as ID_de_pedido_T','ID_Producto','Fecha_de_pedido','ID_de_cliente','ID_de_vendedor','ID_Tipo_Paquete','Cantidad','Valor_total', 'Impuestos')\n",
    "\n",
    "print((ordenes_tmp.count(),ordenes_tmp.distinct().count()))\n",
    "\n",
    "ordenes_tmp = ordenes_tmp.drop_duplicates()\n",
    "ordenes_tmp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardar_db(dest_db_connection_string, movimientos_df,'Estudiante_111_202315.HechoMov1_Tmp', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cree la tabla de Fecha según el material compartido y adicione el left join al crear la tabla de ordenes para que quede completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+---------------+---------------+------------------+--------+-----------+---------+\n",
      "|ID_de_pedido_T|ID_Cliente_DWH|ID_Ciudad_DWH|ID_Empleado_DWH|ID_Producto_DWH|ID_TipoPaquete_DWH|Cantidad|Valor_total|Impuestos|\n",
      "+--------------+--------------+-------------+---------------+---------------+------------------+--------+-----------+---------+\n",
      "|           463|           461|        29242|              2|            187|                 7|       1|         30|      450|\n",
      "|           463|           461|        29242|              2|              8|                 7|       8|        256|     3840|\n",
      "|           463|           461|        29242|              2|            192|                 7|      10|        130|     1950|\n",
      "|           463|           461|        29242|              2|            121|                 7|       3|         39|      585|\n",
      "|           148|           559|        35925|              6|             75|                 7|      40|       1280|    19200|\n",
      "+--------------+--------------+-------------+---------------+---------------+------------------+--------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# El idPedido representa la dimensión degenerada Pedido\n",
    "# Si hay campos nulos en ordenes_tmp al hacer join por el left outer join no se perderan y se utiliza como comodín un id=0 que debe existir en todas las dimensiones.\n",
    "# Ese comodín representa el registro sin Dato.\n",
    "# Debe adicionarle a todas las tablas el registro con identificador 0, como se muestra para la tabla de clientes\n",
    "# Recuerde que falta incluir el join con la tabla de Fecha para que la tabla quede completa.\n",
    "\n",
    "ordenes = ordenes_tmp.alias('o').join(clientes.alias('cl'), ordenes_tmp.ID_de_cliente == clientes.ID_Cliente_T,'left')\\\n",
    "                    .join(ciudades.alias('ciu'), clientes.ID_CiudadEntrega == ciudades.ID_ciudad_T,'left') \\\n",
    "                    .join(empleados.alias('e'), ordenes_tmp.ID_de_vendedor == empleados.ID_Empleado_T,'left') \\\n",
    "                    .join(paquetes.alias('p'), ordenes_tmp.ID_Tipo_Paquete == paquetes.ID_TipoPaquete_T,'left') \\\n",
    "                    .join(productos.alias('pr'), (ordenes_tmp.ID_Producto == productos.ID_Producto_T) ,'left') \\\n",
    "                    .select([col('o.ID_de_pedido_T'),col('cl.ID_Cliente_DWH'),col('ciu.ID_Ciudad_DWH'),\n",
    "                             col('e.ID_Empleado_DWH'),col('pr.ID_Producto_DWH'),col('p.ID_TipoPaquete_DWH'),\n",
    "                             col('o.Cantidad'),col('o.Valor_total'),col('o.Impuestos')]) \\\n",
    "                    .fillna({'ID_Cliente_DWH': 0, 'ID_Ciudad_DWH': 0, 'ID_Empleado_DWH': 0, 'ID_Producto_DWH': 0,\n",
    "                             'ID_TipoPaquete_DWH': 0})\n",
    "ordenes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+--------------+---------------+---------------+------------------+--------+---------+-----+\n",
      "|ID_de_pedido_T|ID_Ciudad_DWH|ID_Cliente_DWH|ID_Empleado_DWH|ID_Producto_DWH|ID_TipoPaquete_DWH|Cantidad|Impuestos|Total|\n",
      "+--------------+-------------+--------------+---------------+---------------+------------------+--------+---------+-----+\n",
      "|           463|        29242|           461|              2|            187|                 7|       1|      450|   30|\n",
      "|           463|        29242|           461|              2|              8|                 7|       8|     3840|  256|\n",
      "|           463|        29242|           461|              2|            192|                 7|      10|     1950|  130|\n",
      "|           463|        29242|           461|              2|            121|                 7|       3|      585|   39|\n",
      "|           471|         3075|           252|              6|             75|                 7|      50|    24000| 1600|\n",
      "+--------------+-------------+--------------+---------------+---------------+------------------+--------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordenes = ordenes.select('ID_de_pedido_T','ID_Ciudad_DWH','ID_Cliente_DWH','ID_Empleado_DWH','ID_Producto_DWH',\n",
    "                         'ID_TipoPaquete_DWH','Cantidad','Impuestos','Valor_total') \\\n",
    "                    .withColumnRenamed('Valor_total','Total')\n",
    "ordenes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9f10qpB86v9"
   },
   "source": [
    "#### Carga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OJO** Recuerde antes de guardar los datos que la tabla no exista o este vacía, para que no se guarden los mismos datos varias veces y no ocupar más espacio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2075.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 742.0 failed 1 times, most recent failure: Lost task 0.0 in stage 742.0 (TID 1130) (172.23.66.196 executor driver): java.sql.BatchUpdateException: Column 'ID_Proveedor_DWH' cannot be null\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat com.mysql.cj.util.Util.handleNewInstance(Util.java:192)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:167)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:174)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)\r\n\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:728)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.sql.SQLIntegrityConstraintViolationException: Column 'ID_Proveedor_DWH' cannot be null\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)\r\n\t... 16 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:893)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.sql.BatchUpdateException: Column 'ID_Proveedor_DWH' cannot be null\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat com.mysql.cj.util.Util.handleNewInstance(Util.java:192)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:167)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:174)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)\r\n\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:728)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.sql.SQLIntegrityConstraintViolationException: Column 'ID_Proveedor_DWH' cannot be null\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)\r\n\t... 16 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11864\\2553946293.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mguardar_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdest_db_connection_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Estudiante_111_202315.Hecho_Movimiento'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_user\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_psswd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtemp_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11864\\211298303.py\u001b[0m in \u001b[0;36mguardar_db\u001b[1;34m(db_connection_string, df, tabla, db_user, db_psswd)\u001b[0m\n\u001b[0;32m     19\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'user'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_user\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'password'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_psswd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m       \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'driver'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'com.mysql.cj.jdbc.Driver'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m       \u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Tutoriales\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 738\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    739\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Tutoriales\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1322\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Tutoriales\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Tutoriales\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2075.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 742.0 failed 1 times, most recent failure: Lost task 0.0 in stage 742.0 (TID 1130) (172.23.66.196 executor driver): java.sql.BatchUpdateException: Column 'ID_Proveedor_DWH' cannot be null\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat com.mysql.cj.util.Util.handleNewInstance(Util.java:192)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:167)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:174)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)\r\n\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:728)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.sql.SQLIntegrityConstraintViolationException: Column 'ID_Proveedor_DWH' cannot be null\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)\r\n\t... 16 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:893)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.sql.BatchUpdateException: Column 'ID_Proveedor_DWH' cannot be null\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat com.mysql.cj.util.Util.handleNewInstance(Util.java:192)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:167)\r\n\tat com.mysql.cj.util.Util.getInstance(Util.java:174)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:853)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:435)\r\n\tat com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:795)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:728)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.sql.SQLIntegrityConstraintViolationException: Column 'ID_Proveedor_DWH' cannot be null\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:117)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1098)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchSerially(ClientPreparedStatement.java:832)\r\n\t... 16 more\r\n"
     ]
    }
   ],
   "source": [
    "guardar_db(dest_db_connection_string, temp_df,'Estudiante_111_202315.Hecho_Movimiento', db_user, db_psswd)#\n",
    "temp_df.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = obtener_dataframe_de_bd(dest_db_connection_string, \"Estudiante_111_202315.Hecho_Movimiento\", db_user, db_psswd)\n",
    "test.show(5)\n",
    "print(f'Numero de registros {test.count()}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXk0Fxmu86v9"
   },
   "source": [
    "Verifique los resultados usando MySQL Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWmcDXue86v9"
   },
   "source": [
    "# Resultado de consultas\n",
    "Corresponde a las consultas realizadas sobre las tablas, para mostrar el estado final de las tablas pobladas como resultado del proceso de ETL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGMlLaje86v9"
   },
   "source": [
    "# 3. Tarea ETL\n",
    "Espacio para desarrollar la tarea planteada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwyeLpt386v9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euvEeguv86v9"
   },
   "source": [
    "## 4. Cierre\n",
    "Completado este tutorial, ya sabe cómo realizar ETL básicos en PySpark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaFNpgeW86v9"
   },
   "source": [
    "## 5. Información adicional\n",
    "\n",
    "Si quiere conocer más sobre PySpark la guía más detallada es la documentación oficial, la cual puede encontrar acá: https://spark.apache.org/docs/latest/api/python/index.html <br>\n",
    "Para ir directamente a la documentación de PySpark SQL, donde está la información sobre los DataFrames, haga clic en este enlace: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html <br>\n",
    "\n",
    "El Capítulo 2 del libro <i>Learn PySpark : Build Python-based Machine Learning and Deep Learning Models, New York: Apress. 2019</i> de Pramod Singh contiene muchos ejemplos útiles, puede encontrarlo en la biblioteca virtual de la universidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kIcwffv86v-"
   },
   "source": [
    "## 6. Preguntas frecuentes\n",
    "\n",
    "- Si al intentar escribir un <i>dataframe</i> obtiene un error en el formato: \n",
    "    ```\n",
    "    path file:<PATH>/dw/<PATH> already exists.;\n",
    "    ```\n",
    "    Borre la carpeta indicada en el error y vuelva a intentar.\n",
    "\n",
    "- Si al ejecutar su código obtiene el error: \n",
    "    ```\n",
    "    ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=tutorial ETL PySpark, master=local) created by __init__ at <ipython-input-4-64455da959dd>:92 \n",
    "\n",
    "    ```\n",
    "    reinicie el kernel del notebook y vuelva a intentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6J9jYwEJ86v-"
   },
   "outputs": [],
   "source": [
    "SPARK.S"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "vtE61qk986vx",
    "X9Oou0g986vy",
    "B5kFkHTD86vz",
    "ddFhEOmL86vz",
    "MuvVgJ4R86v0",
    "BZjDeVYd86v1",
    "_7xbgfCk86v1",
    "Rh_102Yy86v1",
    "HogvCqW_86v2",
    "9rKmT9jd86v2",
    "R9SnkMUH86v3",
    "k8O1GvOd86v3",
    "Mo633Vpg86v3",
    "i36NawhX86v4",
    "LvgnsPfK86v4",
    "dTv_CIOT86v5",
    "N9f10qpB86v9"
   ],
   "name": "MISW-ETL-TutorialETL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
